---
title: "p8105_hw3_jz3036"
output: github_document
author: "Junyuan Zheng (jz3036)"
date: 2018-10-10
---

* Import necessary packages.
```{r import_packages}
library(tidyverse)
#library(readxl)
```

# Problem 1

* Import the raw data for Problem 1.
```{r data_import_p1}
library(p8105.datasets)
data(brfss_smart2010)
```

* Data manipulation:
```{r data_manipulation_p1}
data_p1 = 
  janitor::clean_names(brfss_smart2010) %>%
  filter(., topic == 'Overall Health') %>%
  filter(., response == 'Poor' | response == 'Fair' | response == 'Good' | response == 'Very good' | response == 'Excellent') %>%
  mutate(., response = factor(response, levels = str_c(c("Excellent", "Very good", "Good", "Fair", "Poor"))))
```

* Q1 In 2002, which states were observed at 7 locations?
```{r p1_q1}
data_p1_q1 = 
  filter(data_p1, year == '2002') %>% 
  group_by(., locationabbr) %>% 
  summarize(., n_loc = n_distinct(locationdesc)) %>% 
  filter(., n_loc == 7)
data_p1_q1
```
CT, FL, and NC were observed at 7 locations.

* Q2 Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.
```{r p1_q2}
data_p1 %>% 
  group_by(., year, locationabbr) %>% 
  summarize(., n_loc = n_distinct(locationdesc)) %>% 
  ggplot(aes(x = year, y = n_loc, color = locationabbr)) +
    geom_point() + geom_line() + 
    theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5)) +
    labs(y = 'number of loc', color = 'states', title = 'spaghetti plot')
```

* Q3 Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.
```{r p1_q3}
data_p1 %>%
  filter(., year=='2002' | year=='2006' | year=='2010', response=='Excellent') %>% 
  group_by(., year) %>% 
  summarize(., mean = mean(data_value, na.rm = TRUE), SD = sd(data_value, na.rm = TRUE))
```

* Q4 For each year and state, compute the average proportion in each response category (taking the average across locations in a state).
```{r p1_q4_1}
data_p1_q4_1 = 
  group_by(data_p1, year, locationabbr) %>% 
  summarize(., n_loc = n_distinct(locationdesc))

data_p1_q4_2 =
  group_by(data_p1, year, locationabbr, response) %>%
  summarize(., mean_sum = sum(data_value))

data_p1_q4 = left_join(data_p1_q4_2, data_p1_q4_1, by = c('year'='year', 'locationabbr'='locationabbr'))

data_p1_q4 = mutate(data_p1_q4, avg_prop = mean_sum/n_loc)
head(data_p1_q4)
```

* Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.
```{r p1_q4_2}
ggplot(data_p1_q4, aes(x = year, y = avg_prop, color = locationabbr)) +
    geom_point() + geom_line() +
    facet_grid(. ~ response) +
    theme(legend.position = "right", plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle=90)) +
    labs(y = 'avg_prop', color = 'states', title = 'state-level averages over time')
```

# Problem 2

* Import the raw data for Problem 2.
```{r data_import_p2}
library(p8105.datasets)
data(instacart)
data_p2 = instacart
```

* write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. 

* This is a subset of a dataset containing online purchase information for each order. The dataset we are using contains `r dim(data_p2)[1]` rows or purchase of goods, and `r dim(data_p2)[2]` variables for each purchase.
* The variables can be roughly separated into three parts: cumtomer info, goods info, and time info. Variables that could potientially provides useful infomation include the reordered info, order time of the week and the day, and the aisle info.
* For example, the plot below told us people tend to order yogurt on Sunday more, less on the middle of the week.

```{r p2_example}
data_p2 %>%
  filter(., aisle == 'yogurt') %>% 
  group_by(., order_dow) %>% 
  summarize(., n = n()) %>% 
  ggplot(., aes(x = order_dow, y = n)) +
    geom_point() + geom_line() +
    labs(y = '# of orders', x = 'day of a week', title = 'yogurt order trend')
```


* Q1 How many aisles are there, and which aisles are the most items ordered from?
```{r p2_q1}
data_p2_q1 =
  group_by(data_p2, aisle) %>%
  summarize(., n = n()) %>% 
  mutate(aisle_ranking = min_rank(desc(n)))

nrow(data_p2_q1)
filter(data_p2_q1, aisle_ranking == 1)
```
There are `r nrow(data_p2_q1)` different aisles there, in which fresh vegetables are the aisle most ordered from.

* Q2 Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.
```{r p2_q2}
data_p2_q1 = arrange(data_p2_q1, desc(n))
ggplot(data=data_p2_q1, aes(x=aisle_ranking, y=n, fill = factor(aisle, levels = aisle))) +
  geom_bar(stat="identity", position = "dodge") +
  scale_x_continuous(breaks = seq(0, 140, by = 20)) +
  scale_y_continuous(breaks = seq(0, 150000, by = 10000)) +
  labs(y = 'number of items ordered', x = 'ranking of items ordered', fill = 'aisles') +
  theme(legend.position = "none", legend.key.size = unit(0.01, 'line'))
```
The legend that is mean to guide finding corresponding aisle is too large, so I didn't print it out here.
Also, the 95th and 96th ranking was tied, so there is a adding up on the plot.

* Q3 Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
```{r p2_q3}

```

